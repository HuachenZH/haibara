{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/cityai/deep-learning-for-natural-language-processing-part-i-8369895ffb98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import show, figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/huachen/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/huachen/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    }
   ],
   "source": [
    "# Punctuation and Tokenizer module\n",
    "nltk.download('punkt')\n",
    "# The Gutenberg dataset. A set of 18 books we can used to train upon.\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take a look of books inside gutenberg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to lack of resources, I'm not working with the full Gutenberg \n",
    "# dataset (18 books). If you got a GPU, you can just omit the\n",
    "# ‘fileids’ parameter and all 18 books will be loaded.\n",
    "gberg_sents = gutenberg.sents(fileids=['bible-kjv.txt',\n",
    "                              'austen-emma.txt',\n",
    "                              'austen-persuasion.txt',\n",
    "                              'austen-sense.txt',\n",
    "                              'carroll-alice.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many words are there in the set we loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48304\n"
     ]
    }
   ],
   "source": [
    "print(len(gutenberg.sents(fileids=['bible-kjv.txt',\n",
    "                                   'austen-emma.txt',\n",
    "                                   'austen-persuasion.txt',\n",
    "                                   'austen-sense.txt',\n",
    "                                   'carroll-alice.txt'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hardcore stuffs\n",
    "run the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26663274 -0.21826315 -0.1348525  -0.4016196  -0.52962965  0.03369028\n",
      "  0.09285289 -0.01911975 -0.06512941 -0.12755392  0.21094711  0.04094973\n",
      " -0.0474777  -0.09716108 -0.03971877 -0.01073876 -0.15421328 -0.3395195\n",
      " -0.08078054 -0.12460786 -0.1376849  -0.07534534  0.21138014 -0.3128426\n",
      " -0.0471597  -0.13127126 -0.11695661  0.32264823 -0.32070845 -0.23523596\n",
      " -0.46685365  0.02009837 -0.01898701  0.1418586  -0.21016406 -0.05596342\n",
      " -0.3043874  -0.16800652 -0.20094025  0.2239185  -0.06167073 -0.14520763\n",
      "  0.14730707 -0.06095502  0.42102984  0.08698533 -0.4569305   0.13173863\n",
      "  0.10498156 -0.32804     0.03641748  0.16695778 -0.03016772 -0.07994103\n",
      "  0.37708336  0.4199998  -0.06383915  0.39205605  0.1604757   0.39924726\n",
      " -0.08539315 -0.06614196 -0.7299913  -0.33521625 -0.1659048   0.11154507\n",
      "  0.3070444  -0.40802354  0.3381951  -0.15096235  0.1470864   0.07025782\n",
      "  0.41416436  0.23269054  0.15718628 -0.19619706  0.2164799  -0.83695626\n",
      "  0.08877262 -0.01090841  0.2640706  -0.07118145 -0.22570136 -0.03685217\n",
      "  0.02782664 -0.18237089 -0.1930585  -0.0218869   0.0437368   0.27280474\n",
      "  0.22651169  0.14212468  0.35664892  0.21973792  0.42298952  0.05998357\n",
      " -0.3070849  -0.24190727 -0.30771235 -0.27078307]\n",
      "[('repair', 0.5893843173980713), ('courts', 0.5756785869598389), ('breaches', 0.5601807236671448), ('chamberlain', 0.5567305088043213), ('chamber', 0.5514592528343201), ('banquet', 0.5482771992683411), ('signed', 0.5346192717552185), ('collection', 0.5282104015350342), ('palace', 0.5280013084411621), ('Enter', 0.527736246585846)]\n",
      "[('morning', 0.662590503692627), ('feasting', 0.6221544146537781), ('night', 0.6102601885795593), ('month', 0.605430543422699), ('afternoon', 0.6036787033081055), ('week', 0.5923995971679688), ('sabbath', 0.5910265445709229), ('Abib', 0.5897473096847534), ('hour', 0.585192084312439), ('inn', 0.5838209986686707)]\n",
      "[('mother', 0.762717604637146), ('sister', 0.7150527238845825), ('uncle', 0.6727848052978516), ('younger', 0.660413920879364), ('brother', 0.6473145484924316), ('Amnon', 0.6395153999328613), ('daughter', 0.6261807680130005), ('aunt', 0.6247240304946899), ('indulgent', 0.6199618577957153), ('dearly', 0.6116273999214172)]\n",
      "house\n"
     ]
    }
   ],
   "source": [
    "# size = 64, dimensions\n",
    "# sg = 1, use Skip-Gram. If zero, it will use CBOW\n",
    "# window = 10, context words (10 to the left and 10 to the right)\n",
    "# min_count = 5, ignore words with frequency lower than that\n",
    "# seed = 42, the answer to the universe, life and everything.\n",
    "# workers = 2, number of worker threads.\n",
    "model = Word2Vec(sentences=gberg_sents,  sg=1,\n",
    "                 window=10, min_count=5, seed=42,\n",
    "                 workers=2)\n",
    "# Shows the coordinates of the word ‘house’ in the vector space.\n",
    "print(model.wv['house'])\n",
    "print(model.wv.most_similar('house'))\n",
    "print(model.wv.most_similar('day'))\n",
    "print(model.wv.most_similar('father'))\n",
    "print(model.wv.doesnt_match('mother father daughter house'.split()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
